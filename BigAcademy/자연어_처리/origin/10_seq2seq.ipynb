{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_seq2seq.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"65QH_XqkQrQo","colab_type":"code","colab":{}},"source":["# 구글 드라이브와 연결\n","\n","#from google.colab import auth\n","#auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyAmO_Npsjl9","colab_type":"code","colab":{}},"source":["# 경로 설정\n","path = '/content/gdrive/My Drive/pytest/fra-eng/'\n","\n","!ls '/content/gdrive/My Drive/pytest/fra-eng/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cz0mZ-z_tbrX","colab_type":"code","colab":{}},"source":["# 데이터 확인\n","# 코랩의 메모리 용량 문제가 있어 small 데이터로 진행한다.\n","import pandas as pd\n","data = pd.read_csv(path+'fra-eng_small.txt', names=['source', 'target'], sep='\\t', encoding='utf-8')\n","print('data length:', len(data))          # 268\n","print('data type:', type(data))\n","print('data shape:', data.shape)\n","print('data sample:\\n', data.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBWVLdz7F0Sk","colab_type":"code","colab":{}},"source":["print('data.target length:', len(data.target))          # 268\n","print('data.target type:', type(data.target))           # Series\n","print('data.target shape:', data.target.shape)\n","print('data.target sample:\\n', data.target.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hqBL4HgluILh","colab_type":"code","colab":{}},"source":["# 시작부호와 종료부호 부착\n","# 데이터가 모두 3종이 필요하다. source 언어에는 encoder_input 1개, target 언어에는 decoder_input, decoder_target 2개이다\n","# encoder는 source 언어를 그대로 사용하면 되나, decoder는 seq2seq의 사용을 위해 \\t, \\n를 부착해야 한다\n","# decoder_input 데이터의 시작에는 \\t, 문장의 끝에는 \\n를 부착한다\n","# decoder_target 데이터는 \\n만 필요하다\n","# tab을 받았기 때문에 자료가 더 오른쪽에서 출력되는 것을 알 수 있다\n","data.target_input = data.target.apply(lambda x : '\\t'+x+'\\n')\n","data.target_target = data.target.apply(lambda x : x+'\\n')\n","print('\\nnew data sample:\\n', data.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFAidqDOEaPy","colab_type":"code","colab":{}},"source":["print('data.target_input length:', len(data.target_input))          # 268\n","print('data.target_input type:', type(data.target_input))           # Series\n","print('data.target_input shape:', data.target_input.shape)          # (268,)\n","print('data.target_input sample:\\n', data.target_input.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25if0RgvvCcl","colab_type":"code","colab":{}},"source":["# 문장의 길이 maxlen 설정하기\n","# source와 target 문장의 최대 길이를 구한다\n","source_sentence_max_length = data.source.apply(lambda x: len(x)).max()\n","print('source sentence max length: ', source_sentence_max_length)               # 9\n","target_sentence_max_length = data.target_input.apply(lambda x: len(x)).max()-2  # \"\\t, \\n\"의 길이는 제외\n","print('target sentence max length: ', target_sentence_max_length)               # 30\n","\n","max_src_len = source_sentence_max_length\t\t\t                                  # source 문장의 최대 음절 길이로 maxlen을 설정한다\n","max_tar_len = target_sentence_max_length                                        # target 문장의 최대 음절 길이로 maxlen을 설정한다"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeoMTZHPvsAh","colab_type":"code","colab":{}},"source":["# Data Tokenizing\n","# 각 문자 종류에 대하여 숫자값을 배당한다\n","%tensorflow_version 2.x\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# source 언어 Tokenizing\n","tokenizer_source = Tokenizer(num_words=None, char_level=True, lower=False)      # Tokenizer 객체 생성\n","tokenizer_source.fit_on_texts(data.source)     \t                                # 인덱스를 구축한다\n","word_index_source = tokenizer_source.word_index                                 # 글자와 인덱스의 쌍을 가져온다\n","\n","print('\\n전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index_source))   # 91\n","print('word_index_source: ', word_index_source)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7d6GQDjz1VG6","colab_type":"code","colab":{}},"source":["# target 언어 Tokenizing\n","# target 언어로는 target_input으로 하나만 만들면 된다\n","tokenizer_target = Tokenizer(num_words=None, char_level=True, lower=False)      # Tokenizer 객체 생성\n","tokenizer_target.fit_on_texts(data.target_input)     \t                          # 인덱스를 구축한다\n","word_index_target = tokenizer_target.word_index                                 # 글자와 인덱스의 쌍을 가져온다\n","\n","print('\\n전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index_target))   # 116\n","print('word_index_target: ', word_index_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_ZlLd6pKXj2","colab_type":"code","colab":{}},"source":["# Data Sequencing\n","# 배당된 숫자를 이용하여 각 문장의 문자를 숫자로 치환한다\n","# source 언어 Sequencing\n","# 문제가 있으면 오른쪽과 같이 시작한다. encoder_input = tokenizer_source.texts_to_sequences(list(data.source)) \n","encoder_input = tokenizer_source.texts_to_sequences(data.source) \n","\n","print('\\nResult of encoder_input sequencing: ')\n","print(data.source[0], encoder_input[0])\n","print(data.source[1], encoder_input[1])\n","print(data.source[2], encoder_input[2])\n","print(data.source[3], encoder_input[3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDENb4r024wB","colab_type":"code","colab":{}},"source":["# target 언어 Sequencing\n","decoder_input = tokenizer_target.texts_to_sequences(data.target_input)\n","decoder_target = tokenizer_target.texts_to_sequences(data.target_target)\n","\n","print('\\nResult of decoder_input sequencing: ')\n","print(data.target_input[0], decoder_input[0])\n","print(data.target_input[1], decoder_input[1])\n","print(data.target_input[2], decoder_input[2])\n","\n","print('\\nResult of decoder_target sequencing: ')\n","print(data.target_target[0], decoder_target[0])\n","print(data.target_target[1], decoder_target[1])\n","print(data.target_target[2], decoder_target[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4P5-HskIpqt","colab_type":"code","colab":{}},"source":["print('data.source type:', type(data.source))              # Series\n","print('encoder_input type:', type(encoder_input))          # list\n","\n","print('data.source:\\n', data.source)\n","print('encoder_input\\n:', encoder_input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCj3pmY54NWn","colab_type":"code","colab":{}},"source":["# Data Padding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n","decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n","decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n","\n","print('\\nPadding result sample: ')\n","print(data.target_input[0], decoder_input[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxEfVgbPIrAv","colab_type":"code","colab":{}},"source":["print('decoder_input length:', len(decoder_input))          # 268\n","print('decoder_input type:', type(decoder_input))           # numpy.ndarray\n","print('decoder_input shape:', decoder_input.shape)          # (268, 30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GEj8A1qpWjC0","colab":{}},"source":["# One-Hot-Encoding\n","# 케라스 원-핫 인코딩을 수행한다.\n","# 클래스의 수는 1을 올려주어야 한다(Padding으로 생긴 0을 추가로 받아야 함)\n","from tensorflow.keras.utils import to_categorical\n","encoder_input = to_categorical(encoder_input, num_classes=len(word_index_source)+1)\n","decoder_input = to_categorical(decoder_input, num_classes=len(word_index_target)+1)\n","decoder_target = to_categorical(decoder_target, num_classes=len(word_index_target)+1)\n","\n","print('\\nResult of One-Hot Encodded decoder_input sequencing: ')\n","print(decoder_input.shape)\n","print(data.target_input[0], decoder_input[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rW450oQ6649k","colab_type":"code","colab":{}},"source":["print('0-0\\n', decoder_input[0][0])\n","print('\\n0-1\\n', decoder_input[0][1])\n","print('\\n0-2\\n', decoder_input[0][2])\n","print('\\n0-28\\n', decoder_input[0][28])\n","print('\\n0-29\\n', decoder_input[0][29])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvBLQ2XdBEv4","colab_type":"code","colab":{}},"source":["print('decoder_input length:', len(decoder_input))          # 268\n","print('decoder_input type:', type(decoder_input))           # numpy.ndarray\n","print('decoder_input shape:', decoder_input.shape)          # (268, 30, 70)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrsB_1T-BUqE","colab_type":"code","colab":{}},"source":["# 교사 강요를 이용한 모델 훈련\n","# 예측 과정에서는 이전 시점의 decoder_input의 예측 결과를 현재 시점의 decoder_input으로 넣을 것이다\n","# 그러나 훈련 과정에서는 그렇게 하면 잘못 예측된 이전 시점의 결과가 현재 시점으로 들어가게 된다\n","# 따라서 훈련 과정에서는 이전 시점의 decoder_input의 실제값을 현재 시점의 decoder_input으로 넣을 것이다\n","# 이를 교사 강요라고 한다\n","# 현재 Input이 3D이므로 Embedding으로 차원을 늘려주지 않아도 바로 3D input shape을 갖는 순환신경망을 이용할 수 있다\n","\n","# Context Vector 만들기\n","# 훈련용 Encoder\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense\n","\n","encoder_inputs = Input(shape=(None, len(word_index_source)+1))      # 입력문의 길이가 문장마다 다르므로 None. 출력되는 내용은 최대 len(word_index_source)+1\n","encoder_lstm = LSTM(units=256, return_state=True)                   # encoder 내부 상태를 decoder로 넘겨주기 위해 return_state=True\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)    # return_state=True로 만들어진 모델이므로 은닉상태와 셀상태를 받는다. encoder_outputs는 사용하지 않는다\n","encoder_states = [state_h, state_c]                                 # 은닉상태와 셀상태를 받는다"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8JoHKMsRIS8e","colab_type":"code","colab":{}},"source":["# 훈련용 Decoder\n","decoder_inputs = Input(shape=(None, len(word_index_target)+1))\n","decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)            # 256 unit으로 된 encoder_states를 받아야 하므로 decoder의 은닉 상태도 256으로 동일하게 맞춰준다\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)  # decoder의 첫 상태를 encoder의 은닉 상태와 셀 상태로 한다\n","decoder_dense = Dense(len(word_index_target)+1, activation='softmax')               # decoder의 은닉상태와 셀상태는 훈련 과정에서는 사용하지 않는다(_)\n","decoder_outputs = decoder_dense(decoder_outputs)                                    # 출력층의 크기는 번역문의 글자(또는 단어)가 가질 수 있는 크기이다"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oo-z8pk6Bg-X","colab_type":"code","colab":{}},"source":["# 모델 훈련\n","# full data를 이용하여 epochs를 50회 정도 주어야 제대로 결과가 나온다.\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=1, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9DZLa1jjC9my","colab_type":"code","colab":{}},"source":["# 예측용 Encoder\n","# 입력된 문장을 인코더에 넣어서 은닉상태와 셀상태를 얻는다\n","# encoder_inputs, encoder_states는 훈련용에서 구성한 것을 사용한다\n","encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-g6-bCmlgBR3","colab_type":"code","colab":{}},"source":["# 예측용 Decoder\n","# Decoder가 산출한 결과를 받는 상태 벡터를 정의\n","decoder_state_input_h = Input(shape=(256,))\n","decoder_state_input_c = Input(shape=(256,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","# 문장의 다음 단어를 예측하기 위해서 은닉상태와 셀상태를 받음\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XmH6P_heDkbk","colab_type":"code","colab":{}},"source":["# word로부터 idx를 얻는 것을 idx로부터 word를 얻는 것으로 바꿈\n","index_to_src = dict((i, char) for char, i in word_index_source.items())\n","index_to_tar = dict((i, char) for char, i in word_index_target.items())\n","print(index_to_tar)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yd_AXW0DmmY","colab_type":"code","colab":{}},"source":["def decode_sequence(input_seq):\n","    states_value = encoder_model.predict(input_seq)           # 입력문을 인코더에 넣어 문장의 상태 벡터를 얻는다    \n","    target_seq = np.zeros((1, 1, len(word_index_target)+1))   # 디코더를 초기화한다\n","    target_seq[0, 0, word_index_target['\\t']] = 1.            # 디코더의 첫 시작은 <\\t>로 시작하므로 \\t 위치에 원-핫 인코딩으로 기록\n","    \n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    \n","    #stop_condition이 True가 될 때까지 루프 반복\n","    while not stop_condition:\n","      output_tokens, h, c = decoder_model.predict([target_seq] + states_value)  # target_input과 문장의 상태 벡터 입력\n","      sampled_token_index = np.argmax(output_tokens)          # 70개의 결과 중 가장 큰 값을 갖는 인덱스를 선택\n","      \n","      # sampled_token_index가 0이 나오면, index_to_tar 0은 없으므로 에러가 발생한다.\n","      # 이 경우 공백을 갖는 index 1로 치환한다.\n","      # 대량의 데이터에서는 확률상 0이 나오지 않는다\n","      # 소량의 데이터에서는 대부분 0이 나온다.\n","      # 소량의 데이터에서 index_to_tar 0으로 치우치는 이유는, 1~69 어떠한 것도 확률이 낮기 때문에\n","      # 전체의 합을 1로 만들기 위해서, 빈 영역인 0번에 가장 높은 값이 들어가기 때문이다.\n","      # 즉, 0으로 나왔다는 것은 예측기가 어떠한 음절도 가능성이 낮은 것으로 본다 라고 해석할 수 있다\n","      if(sampled_token_index==0):  \n","        sampled_token_index = 1\n","        \n","      sampled_char = index_to_tar[sampled_token_index]\n","      decoded_sentence += sampled_char\n","\n","      # 종료표시인 <\\n>에 도달하거나 최대 길이를 넘으면 중단.\n","      if (sampled_char == '\\n' or len(decoded_sentence) > max_tar_len):\n","        stop_condition = True\n","\n","      # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n","      target_seq = np.zeros((1, 1, len(word_index_target)+1))   # target_input 초기화\n","      target_seq[0, 0, sampled_token_index] = 1.                # 직전 예측 결과를 sampled_token_index 위치에 원-핫 인코딩으로 기록\n","\n","      # 상태를 업데이트 합니다.\n","      states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aA4XzA7eDqdT","colab_type":"code","colab":{}},"source":["import numpy as np\n","for seq_index in [4, 51, 101]: # 입력 문장의 인덱스\n","  input_seq = encoder_input[seq_index:seq_index+1]  # 3차원 배열에서는 이와 같이 [n:n+1] 형태로 해주어야 3차원이 유지되면서 n번째가 출력된다\n","  decoded_sentence = decode_sequence(input_seq)\n","  \n","  print(35 * \"-\")\n","  print('입력 문장:', data.source[seq_index])\n","  print('정답 문장:', data.target[seq_index][:len(data.target[seq_index])])\n","  print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])    # '\\n'은 빼고 출력"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ms644xw_8g27","colab_type":"code","colab":{}},"source":["# 1개 문장 예측하기\n","# 전처리\n","input_seq_1 = tokenizer_source.texts_to_sequences([list('Run!')])\n","input_seq_1 = pad_sequences(input_seq_1, maxlen=max_src_len, padding='post')\n","input_seq_1 = to_categorical(input_seq_1, num_classes=len(word_index_source)+1)\n","\n","# 예측하기 \n","decoded_sentence = decode_sequence(input_seq_1)\n","print(decoded_sentence)"],"execution_count":null,"outputs":[]}]}