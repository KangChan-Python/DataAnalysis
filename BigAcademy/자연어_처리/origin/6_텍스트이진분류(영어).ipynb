{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_텍스트이진분류(영어).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNZyzW6tFV9I3nYbZRwUKLo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OXEF_Ue7pf7F","colab_type":"text"},"source":["# 텍스트 이진분류 (영어 IMDB)"]},{"cell_type":"code","metadata":{"id":"zxxo2kYPpbfd","colab_type":"code","colab":{}},"source":["# from google.colab import auth\n","# auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUsarwHDpfhj","colab_type":"code","colab":{}},"source":["# 경로 설정\n","imdb_dir = '/content/gdrive/My Drive/pytest/aclImdb_v1_small/aclImdb/'\n","\n","!ls '/content/gdrive/My Drive/pytest/aclImdb_v1_small/aclImdb/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"33h_t_NHpmb-","colab_type":"code","colab":{}},"source":["# Train Data Loading\n","import os\n","train_dir = os.path.join(imdb_dir, 'train')\n","labels = [] ; texts = []\n","\n","for label_type in ['neg', 'pos']:\n","  dir_name = os.path.join(train_dir, label_type)\n","  for fname in os.listdir(dir_name):\n","    if fname[-4:] == '.txt':\n","      f = open(os.path.join(dir_name, fname), encoding='utf8')\n","      texts.append(f.read())\n","      f.close()\n","      if label_type == 'neg':\n","        labels.append(0)\n","      else:\n","        labels.append(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yftvnDZPpoFw","colab_type":"code","colab":{}},"source":["# Data 확인\n","print('texts 0:', texts[0])\n","print('texts len:', len(texts))\n","\n","print('labels 0:', labels[0])\n","print('labels len:', len(labels))\n","\n","print('texts type:', type(texts))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JV_RIeFppjy","colab_type":"code","colab":{}},"source":["# Tokenizer 연습\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","max_words = 10000\n","\n","sample1 = [['사과 감자 옥수수'], ['딸기 감자 옥수수'], ['양파 감자 옥수수'], ['양파 부추 옥수수']]\n","sample2 = [['사과', '감자', '옥수수'], ['너희', '감자', '옥수수'], ['그들', '감자', '옥수수'], ['양파', '부추', '옥수수']]\n","sample3 = ['사과 감자 옥수수 너희 그들 양파 부추']\n","sample4 = ['사과', '감자', '옥수수', '너희', '그들', '양파', '부추']\n","\n","tokenizer = Tokenizer(num_words=max_words)\t      # Tokenizer 객체 생성\n","tokenizer.fit_on_texts(sample1)     \t\t\t        # 단어 인덱스를 구축한다 \n","word_index = tokenizer.word_index           \t\t  # 단어 인덱스만 가져온다\n","print(word_index)\n","\n","tokenizer = Tokenizer(num_words=max_words)\t      # Tokenizer 객체 생성\n","tokenizer.fit_on_texts(sample2)     \t\t\t        \n","word_index = tokenizer.word_index           \t\t  \n","print(word_index)\n","\n","tokenizer = Tokenizer(num_words=max_words)\t      # Tokenizer 객체 생성\n","tokenizer.fit_on_texts(sample3)     \t\t\t         \n","word_index = tokenizer.word_index           \t\t  \n","print(word_index)\n","\n","tokenizer = Tokenizer(num_words=max_words)\t      # Tokenizer 객체 생성\n","tokenizer.fit_on_texts(sample4)     \t\t\t         \n","word_index = tokenizer.word_index           \t\t  \n","print(word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZPuARLLpr2k","colab_type":"code","colab":{}},"source":["# Data Tokenizing\n","# 텍스트에 사용된 단어의 종류를 빈도 순으로 정렬하는 작업을 수행한다\n","%tensorflow_version 2.x\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import math\n","\n","validation_ratio = math.floor(len(texts) * 0.3)  \t# 검증 샘플은 전체의 30%로 한다\n","max_words = 10000               \t\t\t            # 데이터셋에서 가장 빈도 높은 10,000 개의 단어만 사용한다\n","maxlen = 200\t\t\t\t\t                            # 항상 200 단어가 되도록 길이를 고정한다\n","\n","tokenizer = Tokenizer(num_words=max_words)\t      # 상위빈도 10,000 개의 단어만을 추려내는 Tokenizer 객체 생성\n","tokenizer.fit_on_texts(texts)     \t\t\t          # 단어 인덱스를 구축한다 \n","word_index = tokenizer.word_index           \t\t  # 단어 인덱스만 가져온다 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnqpX9m2ptQO","colab_type":"code","colab":{}},"source":["# Tokenizing 결과 확인\n","print('전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n","print('word_index type: ', type(word_index))\n","print('word_index: ', word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"os-5gijUpujg","colab_type":"code","colab":{}},"source":["# Data Sequencing\n","# 문자를 숫자로 변환하는 작업을 수행한다\n","# 상위 빈도 10,000(max_words)개의 단어만 추출하여 word_index의 숫자 리스트로 변환한다.\n","data = tokenizer.texts_to_sequences(texts)\t\t# Tokenizer 결과가 여기서 반영된다.\n","\n","print('data 0:', data[0])\n","print('texts 0:', texts[0])\t\t\t\t           # texts[0]의 본래 단어들"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CAZJMF8cpv02","colab_type":"code","colab":{}},"source":["print(type(texts))\n","print(type(data))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbnxqSKWpxOZ","colab_type":"code","colab":{}},"source":["# Data Pading 연습\n","from keras.preprocessing.sequence import pad_sequences\n","\n","sequences = [[1, 2, 3, 4, 5], [1, 2, 3, 4], [1]]\t\t  # nested list\n","padded = pad_sequences(sequences, maxlen=3)\t\t  # 2D tensor \n","print(padded)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2bw-2W0pyka","colab_type":"code","colab":{}},"source":["# Data Pading\n","data = pad_sequences(data, maxlen=maxlen) \n","\n","print('data:', data)\n","print('data 0:', data[0])\n","print(len(data[0]))\n","\n","print(word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lCjsDE4Rpz02","colab_type":"code","colab":{}},"source":["print(type(texts))\n","print(type(data))\n","print(data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-16bHV2p1TB","colab_type":"code","colab":{}},"source":["# One-Hot-Encoding 연습\n","sample = [[5, 6, 7], [8, 9, 10]]\n","arr = np.zeros((len(sample), 10+1))\t\t# “10”은 11번째에 들어가게 되므로 11개의 공간을 만들어야 한다\n","for i, seq in enumerate(sample):\t\t  # 리스트가 2개이므로 i는 총 2회(0, 1) 반복되며,\n","   \tarr[i, seq] = 1.\t\t\t\t          # 각 i에서 리스트의 number가 가리키는 곳에 1을 기록한다\n","arr"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qjbHWtUp26p","colab_type":"code","colab":{}},"source":["# One-Hot-Encoding\n","def to_one_hot(sequences, dimension):\n","  results = np.zeros((len(sequences), dimension))\n","  for i, sequence in enumerate(sequences):\n","    results[i, sequence] = 1.\n","  return results\n","\n","\n","data = to_one_hot(data, dimension=max_words) \n","labels = np.asarray(labels).astype('float32')   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GK4LmGMNp4Hm","colab_type":"code","colab":{}},"source":["# One-Hot-Encoding 결과 확인\n","print('data:', data)\n","print(len(data[0]))\t\t\t\t\t# dimension=10000으로 했으므로 각 행은 10,000개를 가지고 있다\n","print('data [0][0:100]:', data[0][0:100])\n","\n","print(word_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGDW4jKgp5U0","colab_type":"code","colab":{}},"source":["print(type(texts))\n","print(type(data))\n","print(data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ys4jthhdp6Zk","colab_type":"code","colab":{}},"source":["# Train 데이터와 Validation 데이터 준비\n","\n","print('데이터 텐서의 크기:', data.shape)  \t\t# (25000, 10000)\n","print('레이블 텐서의 크기:', labels.shape) \t\t# (25000,) data와 label이 모두 2D 텐서가 되었음\n","\n","indices = np.arange(data.shape[0]) \t\t        # 0 ~ 24999 까지의 숫자를 생성\n","np.random.shuffle(indices)     \t\t\t          # 0 ~ 24999 까지의 숫자를 랜덤하게 섞음\n","data = data[indices]    \t\t\t\t              # 이것을 인덱스로 하여 2D 텐서 데이터를 섞음 \n","labels = labels[indices]\t\t\t\t              # label도 같은 순서로 섞음 \n","\n","print(indices)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FqrxZcFp7nn","colab_type":"code","colab":{}},"source":["# 훈련데이터와 검증데이터 분리\n","x_train = data[validation_ratio:] \t\t\t      # 훈련데이터의 70%를 훈련데이터\n","y_train = labels[validation_ratio:] \t\t\t    # 훈련데이터의 70%를 훈련데이터 Label (data와 labels는 같은 순서)\n","x_val = data[:validation_ratio] \t\t\t        # 훈련데이터의 30%를 검증데이터\n","y_val = labels[:validation_ratio] \t\t\t      # 훈련데이터의 30%를 검증데이터 Label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpO_0ostp85w","colab_type":"code","colab":{}},"source":["# 모델 정의하기\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = Sequential()                                              # 모델을 새로 정의\n","\n","model.add(Dense(64, activation='relu', input_shape=(max_words,)))\t# 첫 번째 은닉층\n","model.add(Dense(32, activation='relu'))                           # 두 번째 은닉층\n","model.add(Dense(1, activation='sigmoid'))                 \t\t    # 출력층"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8Q9aat2p-Rx","colab_type":"code","colab":{}},"source":["# 모델 요약 출력\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UwXIMjQRqAFm","colab_type":"code","colab":{}},"source":["# Compile & Train Model\n","# 모델 컴파일\n","model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n","\n","# 모델 훈련\n","history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n","history_dict = history.history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPqy_dSnqMwY","colab_type":"code","colab":{}},"source":["# 경로 변경\n","%cd /content/gdrive/My Drive/pytest/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bKYKN-LqOlQ","colab_type":"code","colab":{}},"source":["# Save Model\n","# multidimensional numpy arrays를 저장할 수 있는 h5 file(HDF) 포맷으로 저장한다\n","model.save('text_binary_model.h5')\n","\n","\n","# 훈련데이터에서 사용된 상위빈도 10,000개의 단어로 된 Tokenizer 저장\n","# 새로 입력되는 문장에서도 같은 단어가 추출되게 한다\n","import pickle\n","with open('text_binary_tokenizer.pickle', 'wb') as handle:\n","  pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vfJowCJqQD7","colab_type":"code","colab":{}},"source":["# Accuracy & Loss 확인\n","# history 딕셔너리 안에 있는 정확도와 손실값을 가져와 본다\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","print('Accuracy of each epoch:', acc)\t\t# [0.79, 0.90, 0.93, 0.94, 0.96, 0.97, 0.98, 0.98, 0.98, 0.99]\n","epochs = range(1, len(acc) +1)\t\t\t# range(1, 11)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLefyjM7qRUD","colab_type":"code","colab":{}},"source":["# Plotting Accuracy\n","import matplotlib.pyplot as plt\n","\n","# 훈련데이터의 정확도에 비해 검증데이터의 정확도는 낮게 나타난다\n","# epoch가 높아지면 모델은 훈련데이터에 매우 민감해져 오히려 새로운 데이터를 잘 못 맞춘다\n","plt.plot(epochs, acc, 'bo', label='Training Acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation Acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SlGYIVwqSlT","colab_type":"code","colab":{}},"source":["# Plotting Loss\n","plt.figure()    # 새로운 그림을 그린다\n","\n","# 훈련데이터의 손실값은 낮아지나, 검증데이터의 손실값은 높아진다\n","# 손실값은 오류값을 말한다. 예측과 정답의 차이를 거리 계산으로 구한 값이다\n","plt.plot(epochs, loss, 'bo', label='Training Loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VzPLKuUtqUR4","colab_type":"code","colab":{}},"source":["# Load Model\n","import os\n","from tensorflow.keras.models import load_model\n","\n","filepath = '/content/gdrive/My Drive/pytest/'\n","os.chdir(filepath)\n","print(\"Current Directory:\", os.getcwd())\n","\n","loaded_model = load_model('text_binary_model.h5')\n","print(\"model loaded:\", loaded_model)\n","\n","with open('text_binary_tokenizer.pickle', 'rb') as handle:\n","  loaded_tokenizer = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0wS05HhEqVqR","colab_type":"code","colab":{}},"source":["# Test Data Loading\n","test_dir = os.path.join(imdb_dir, 'test')\n","labels = [] ; texts = []\n","\n","for label_type in ['neg', 'pos']:\n","  dir_name = os.path.join(test_dir, label_type)\n","  for fname in os.listdir(dir_name):\n","    if fname[-4:] == '.txt':\n","      f = open(os.path.join(dir_name, fname), encoding='utf8')\n","      texts.append(f.read())\n","      f.close()\n","      if label_type == 'neg':\n","        labels.append(0)\n","      else:\n","        labels.append(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bk8d2KpvqW8n","colab_type":"code","colab":{}},"source":["# Data 확인\n","print('texts:', texts[0])\n","print('texts len:', len(texts))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wsLSrmrqYUC","colab_type":"code","colab":{}},"source":["# Data Sequencing\n","# 문자열을 word_index의 숫자 리스트로 변환\n","data = loaded_tokenizer.texts_to_sequences(texts)\n","\n","# padding으로 문자열의 길이를 고정시킨다\n","data = pad_sequences(data, maxlen=maxlen) \n","\n","# test 데이터를 원-핫 인코딩한다\n","x_test = to_one_hot(data, dimension=max_words)\n","\n","# label을 list에서 넘파이 배열로 변환. 결과가 0 또는 1만 나오므로 이와같이 int32로 저장해도 된다.\n","y_test = np.asarray(labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ce6e6REqaFF","colab_type":"code","colab":{}},"source":["# Test Data Evaluation\n","test_eval = loaded_model.evaluate(x_test, y_test)\t  # 모델에 분류할 데이터와 그 정답을 같이 넣어준다\n","print('prediction model loss & acc:', test_eval)\t\t# 모델이 분류한 결과와 입력된 정답을 비교한 결과"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4MdJ_rXrqbgo","colab_type":"code","colab":{}},"source":["# 1개 데이터 예측\n","text = [\"Hi, this is a test sentence.\"]      # 데이터를 list 타입으로 만든다\n","data = loaded_tokenizer.texts_to_sequences(text)\n","data = pad_sequences(data, maxlen=maxlen)\n","x_test = to_one_hot(data, dimension=max_words)\n","\n","prediction = loaded_model.predict(x_test)\n","print(\"Result:\", prediction)\t\t\t\t\t       # [[0.53135556]]. 1이 될 확률이 53.1%"],"execution_count":0,"outputs":[]}]}